{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two code blocks load different pretrained models to train classifiers for a classification task. The first code block loads a model from vision_transformer, and the second code block loads a resnet50 model. Before running the code, it is necessary to update the path settings in the code based on the file locations.\n",
    "\n",
    "The third code block uses the already trained classifier model to perform some evaluations, calculates the confusion matrix, and saves the result images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m983501138\u001b[0m (\u001b[33mxinliang1001\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c318598b83949bd8bef7332d0064721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112405308004883, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/autodl-tmp/xin/Classify/wandb/run-20240523_190401-x7qcnaby</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xinliang1001/my-ViT-project/runs/x7qcnaby' target=\"_blank\">driven-snowflake-1</a></strong> to <a href='https://wandb.ai/xinliang1001/my-ViT-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xinliang1001/my-ViT-project' target=\"_blank\">https://wandb.ai/xinliang1001/my-ViT-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xinliang1001/my-ViT-project/runs/x7qcnaby' target=\"_blank\">https://wandb.ai/xinliang1001/my-ViT-project/runs/x7qcnaby</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/54210 (0%)]\tLoss: 2.469265\n",
      "Train Epoch: 1 [800/54210 (1%)]\tLoss: 2.210613\n",
      "Train Epoch: 1 [1600/54210 (3%)]\tLoss: 2.483333\n",
      "Train Epoch: 1 [2400/54210 (4%)]\tLoss: 2.513893\n",
      "Train Epoch: 1 [3200/54210 (6%)]\tLoss: 2.306395\n",
      "Train Epoch: 1 [4000/54210 (7%)]\tLoss: 2.270437\n",
      "Train Epoch: 1 [4800/54210 (9%)]\tLoss: 2.402925\n",
      "Train Epoch: 1 [5600/54210 (10%)]\tLoss: 2.222271\n",
      "Train Epoch: 1 [6400/54210 (12%)]\tLoss: 2.250594\n",
      "Train Epoch: 1 [7200/54210 (13%)]\tLoss: 2.303944\n",
      "Train Epoch: 1 [8000/54210 (15%)]\tLoss: 2.293039\n",
      "Train Epoch: 1 [8800/54210 (16%)]\tLoss: 2.233791\n",
      "Train Epoch: 1 [9600/54210 (18%)]\tLoss: 2.281079\n",
      "Train Epoch: 1 [10400/54210 (19%)]\tLoss: 2.283622\n",
      "Train Epoch: 1 [11200/54210 (21%)]\tLoss: 2.509204\n",
      "Train Epoch: 1 [12000/54210 (22%)]\tLoss: 1.976008\n",
      "Train Epoch: 1 [12800/54210 (24%)]\tLoss: 2.011058\n",
      "Train Epoch: 1 [13600/54210 (25%)]\tLoss: 1.715079\n",
      "Train Epoch: 1 [14400/54210 (27%)]\tLoss: 2.011472\n",
      "Train Epoch: 1 [15200/54210 (28%)]\tLoss: 1.906410\n",
      "Train Epoch: 1 [16000/54210 (30%)]\tLoss: 2.122560\n",
      "Train Epoch: 1 [16800/54210 (31%)]\tLoss: 2.149440\n",
      "Train Epoch: 1 [17600/54210 (32%)]\tLoss: 1.814554\n",
      "Train Epoch: 1 [18400/54210 (34%)]\tLoss: 1.985713\n",
      "Train Epoch: 1 [19200/54210 (35%)]\tLoss: 2.054763\n",
      "Train Epoch: 1 [20000/54210 (37%)]\tLoss: 1.880443\n",
      "Train Epoch: 1 [20800/54210 (38%)]\tLoss: 1.862286\n",
      "Train Epoch: 1 [21600/54210 (40%)]\tLoss: 1.387770\n",
      "Train Epoch: 1 [22400/54210 (41%)]\tLoss: 2.023485\n",
      "Train Epoch: 1 [23200/54210 (43%)]\tLoss: 2.092138\n",
      "Train Epoch: 1 [24000/54210 (44%)]\tLoss: 1.803552\n",
      "Train Epoch: 1 [24800/54210 (46%)]\tLoss: 1.990532\n",
      "Train Epoch: 1 [25600/54210 (47%)]\tLoss: 2.005948\n",
      "Train Epoch: 1 [26400/54210 (49%)]\tLoss: 2.176574\n",
      "Train Epoch: 1 [27200/54210 (50%)]\tLoss: 1.843268\n",
      "Train Epoch: 1 [28000/54210 (52%)]\tLoss: 2.272948\n",
      "Train Epoch: 1 [28800/54210 (53%)]\tLoss: 1.751006\n",
      "Train Epoch: 1 [29600/54210 (55%)]\tLoss: 2.103125\n",
      "Train Epoch: 1 [30400/54210 (56%)]\tLoss: 2.182777\n",
      "Train Epoch: 1 [31200/54210 (58%)]\tLoss: 2.081223\n",
      "Train Epoch: 1 [32000/54210 (59%)]\tLoss: 2.114143\n",
      "Train Epoch: 1 [32800/54210 (60%)]\tLoss: 1.570274\n",
      "Train Epoch: 1 [33600/54210 (62%)]\tLoss: 2.178118\n",
      "Train Epoch: 1 [34400/54210 (63%)]\tLoss: 1.943805\n",
      "Train Epoch: 1 [35200/54210 (65%)]\tLoss: 2.191985\n",
      "Train Epoch: 1 [36000/54210 (66%)]\tLoss: 1.608726\n",
      "Train Epoch: 1 [36800/54210 (68%)]\tLoss: 1.962514\n",
      "Train Epoch: 1 [37600/54210 (69%)]\tLoss: 2.223973\n",
      "Train Epoch: 1 [38400/54210 (71%)]\tLoss: 1.869221\n",
      "Train Epoch: 1 [39200/54210 (72%)]\tLoss: 1.897901\n",
      "Train Epoch: 1 [40000/54210 (74%)]\tLoss: 1.670336\n",
      "Train Epoch: 1 [40800/54210 (75%)]\tLoss: 2.194568\n",
      "Train Epoch: 1 [41600/54210 (77%)]\tLoss: 2.257398\n",
      "Train Epoch: 1 [42400/54210 (78%)]\tLoss: 2.004538\n",
      "Train Epoch: 1 [43200/54210 (80%)]\tLoss: 1.502290\n",
      "Train Epoch: 1 [44000/54210 (81%)]\tLoss: 2.066375\n",
      "Train Epoch: 1 [44800/54210 (83%)]\tLoss: 1.609958\n",
      "Train Epoch: 1 [45600/54210 (84%)]\tLoss: 1.868621\n",
      "Train Epoch: 1 [46400/54210 (86%)]\tLoss: 2.348945\n",
      "Train Epoch: 1 [47200/54210 (87%)]\tLoss: 1.885470\n",
      "Train Epoch: 1 [48000/54210 (89%)]\tLoss: 2.140559\n",
      "Train Epoch: 1 [48800/54210 (90%)]\tLoss: 1.721504\n",
      "Train Epoch: 1 [49600/54210 (91%)]\tLoss: 2.066552\n",
      "Train Epoch: 1 [50400/54210 (93%)]\tLoss: 1.762787\n",
      "Train Epoch: 1 [51200/54210 (94%)]\tLoss: 1.780087\n",
      "Train Epoch: 1 [52000/54210 (96%)]\tLoss: 1.475498\n",
      "Train Epoch: 1 [52800/54210 (97%)]\tLoss: 1.711093\n",
      "Train Epoch: 1 [53600/54210 (99%)]\tLoss: 2.152647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2476, Accuracy: 2440/9796 (25%)\n",
      "\n",
      "Precision: 0.2456, Recall: 0.2486\n",
      "Confusion Matrix:\n",
      " [[709   0  26   0  18  33   5   0 189   0]\n",
      " [  3 775   1   0  80   0   0 141   2   0]\n",
      " [506   1  20   0  77  92   5  26 275   0]\n",
      " [481   0  20   0  54 135   7  51 253   1]\n",
      " [147   9  78   0 309 129  21  88 198   3]\n",
      " [362   0  24   0  54 167   7  38 238   2]\n",
      " [353   3  58   0 181  90  12  33 228   0]\n",
      " [ 80  14  64   0 322 132  20 185 182   3]\n",
      " [559   0  28   0  48  70   8   2 259   0]\n",
      " [193   6  70   0 262 165  14  60 228   4]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import vision_transformer\n",
    "from torchvision.models.vision_transformer import vit_b_16\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "import wandb\n",
    "\n",
    "# 初始化 wandb\n",
    "wandb.init(\n",
    "    project=\"my-ViT-project\",\n",
    "    config={\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 8,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 设定设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据转换，适用于彩色图像\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   # 调整图像大小以匹配模型输入\n",
    "    transforms.RandomHorizontalFlip(),  # 随机水平翻转\n",
    "    transforms.RandomRotation(10),  # 随机旋转\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.0439, 0.0438, 0.0438], std=[0.0942, 0.0941, 0.0941]),\n",
    "])\n",
    "\n",
    "# 定义自定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.annotations.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 加载数据集\n",
    "train_csv = '/root/autodl-tmp/xin/datasets/MNIST/updated_train_labels_with_colors.csv'\n",
    "test_csv = '/root/autodl-tmp/xin/datasets/MNIST/updated_test_labels_with_colors.csv'\n",
    "dataset_path = '/root/autodl-tmp/xin/datasets/MNIST/colored-train-images'\n",
    "testdatapath=  \"/root/autodl-tmp/xin/datasets/MNIST/colored-test-images\"\n",
    "train_dataset = CustomDataset(csv_file=train_csv, root_dir=dataset_path, transform=transform)\n",
    "test_dataset = CustomDataset(csv_file=test_csv, root_dir=testdatapath, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "# 加载预训练模型\n",
    "model = vit_b_16(weights=vision_transformer.ViT_B_16_Weights.DEFAULT).to(device)\n",
    "\n",
    "num_features = model.heads[-1].in_features  # 获取最后一个线性层的输入特征数量\n",
    "model.heads[-1] = nn.Linear(num_features, 10).to(device)  # 替换为新的线性层，适配10类\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练函数\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # Logging to wandb\n",
    "        wandb.log({\"Train Loss\": loss.item()})\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "        torch.cuda.empty_cache()  # 尝试在这里清理CUDA缓存\n",
    "\n",
    "# 测试函数\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # 将一批的损失加和\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # 获取概率最高的索引\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    # 计算混淆矩阵及其它统计指标\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds, average='macro')\n",
    "    recall = recall_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    # Logging to wandb\n",
    "    wandb.log({\"Test Loss\": test_loss, \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall})\n",
    "    \n",
    "\n",
    "\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
    "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "    print('Confusion Matrix:\\n', cm)\n",
    "\n",
    "# 保存模型函数\n",
    "def save_model(model, model_name, epoch, path):\n",
    "    torch.save(model.state_dict(), os.path.join(path, f\"{model_name}_epoch{epoch}.pth\"))\n",
    "\n",
    "model_save_path = '/root/autodl-tmp/xin/Classify/modelweight'\n",
    "\n",
    "# 训练和测试模型\n",
    "for epoch in range(1, 2):\n",
    "    train(epoch)\n",
    "    test()\n",
    "    save_model(model, \"vision_transformer\", epoch, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m983501138\u001b[0m (\u001b[33mxinliang1001\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f010037fbe74072a13a28a2a0e141c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112927955885728, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/autodl-tmp/xin/Classify/wandb/run-20240525_200709-2w5a0o73</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xinliang1001/my-ResNet50-project/runs/2w5a0o73' target=\"_blank\">atomic-sea-4</a></strong> to <a href='https://wandb.ai/xinliang1001/my-ResNet50-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xinliang1001/my-ResNet50-project' target=\"_blank\">https://wandb.ai/xinliang1001/my-ResNet50-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xinliang1001/my-ResNet50-project/runs/2w5a0o73' target=\"_blank\">https://wandb.ai/xinliang1001/my-ResNet50-project/runs/2w5a0o73</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/54210 (0%)]\tLoss: 2.224922\n",
      "Train Epoch: 1 [800/54210 (1%)]\tLoss: 2.472673\n",
      "Train Epoch: 1 [1600/54210 (3%)]\tLoss: 2.333783\n",
      "Train Epoch: 1 [2400/54210 (4%)]\tLoss: 2.143037\n",
      "Train Epoch: 1 [3200/54210 (6%)]\tLoss: 2.337216\n",
      "Train Epoch: 1 [4000/54210 (7%)]\tLoss: 2.454393\n",
      "Train Epoch: 1 [4800/54210 (9%)]\tLoss: 2.316315\n",
      "Train Epoch: 1 [5600/54210 (10%)]\tLoss: 2.284156\n",
      "Train Epoch: 1 [6400/54210 (12%)]\tLoss: 2.187891\n",
      "Train Epoch: 1 [7200/54210 (13%)]\tLoss: 2.329399\n",
      "Train Epoch: 1 [8000/54210 (15%)]\tLoss: 2.428216\n",
      "Train Epoch: 1 [8800/54210 (16%)]\tLoss: 2.268041\n",
      "Train Epoch: 1 [9600/54210 (18%)]\tLoss: 2.242153\n",
      "Train Epoch: 1 [10400/54210 (19%)]\tLoss: 2.150630\n",
      "Train Epoch: 1 [11200/54210 (21%)]\tLoss: 2.311295\n",
      "Train Epoch: 1 [12000/54210 (22%)]\tLoss: 2.359179\n",
      "Train Epoch: 1 [12800/54210 (24%)]\tLoss: 2.245708\n",
      "Train Epoch: 1 [13600/54210 (25%)]\tLoss: 2.106628\n",
      "Train Epoch: 1 [14400/54210 (27%)]\tLoss: 2.599459\n",
      "Train Epoch: 1 [15200/54210 (28%)]\tLoss: 2.355271\n",
      "Train Epoch: 1 [16000/54210 (30%)]\tLoss: 1.601209\n",
      "Train Epoch: 1 [16800/54210 (31%)]\tLoss: 2.271349\n",
      "Train Epoch: 1 [17600/54210 (32%)]\tLoss: 1.961697\n",
      "Train Epoch: 1 [18400/54210 (34%)]\tLoss: 2.340039\n",
      "Train Epoch: 1 [19200/54210 (35%)]\tLoss: 1.579653\n",
      "Train Epoch: 1 [20000/54210 (37%)]\tLoss: 2.296118\n",
      "Train Epoch: 1 [20800/54210 (38%)]\tLoss: 1.904601\n",
      "Train Epoch: 1 [21600/54210 (40%)]\tLoss: 2.110150\n",
      "Train Epoch: 1 [22400/54210 (41%)]\tLoss: 1.815791\n",
      "Train Epoch: 1 [23200/54210 (43%)]\tLoss: 2.116294\n",
      "Train Epoch: 1 [24000/54210 (44%)]\tLoss: 2.259405\n",
      "Train Epoch: 1 [24800/54210 (46%)]\tLoss: 1.625309\n",
      "Train Epoch: 1 [25600/54210 (47%)]\tLoss: 2.123007\n",
      "Train Epoch: 1 [26400/54210 (49%)]\tLoss: 1.961354\n",
      "Train Epoch: 1 [27200/54210 (50%)]\tLoss: 1.175041\n",
      "Train Epoch: 1 [28000/54210 (52%)]\tLoss: 1.766549\n",
      "Train Epoch: 1 [28800/54210 (53%)]\tLoss: 1.940254\n",
      "Train Epoch: 1 [29600/54210 (55%)]\tLoss: 2.290079\n",
      "Train Epoch: 1 [30400/54210 (56%)]\tLoss: 1.264892\n",
      "Train Epoch: 1 [31200/54210 (58%)]\tLoss: 1.691422\n",
      "Train Epoch: 1 [32000/54210 (59%)]\tLoss: 1.416899\n",
      "Train Epoch: 1 [32800/54210 (60%)]\tLoss: 1.901730\n",
      "Train Epoch: 1 [33600/54210 (62%)]\tLoss: 1.923340\n",
      "Train Epoch: 1 [34400/54210 (63%)]\tLoss: 2.595532\n",
      "Train Epoch: 1 [35200/54210 (65%)]\tLoss: 1.432806\n",
      "Train Epoch: 1 [36000/54210 (66%)]\tLoss: 1.281731\n",
      "Train Epoch: 1 [36800/54210 (68%)]\tLoss: 1.735526\n",
      "Train Epoch: 1 [37600/54210 (69%)]\tLoss: 1.087209\n",
      "Train Epoch: 1 [38400/54210 (71%)]\tLoss: 0.996318\n",
      "Train Epoch: 1 [39200/54210 (72%)]\tLoss: 1.015905\n",
      "Train Epoch: 1 [40000/54210 (74%)]\tLoss: 1.711999\n",
      "Train Epoch: 1 [40800/54210 (75%)]\tLoss: 1.388376\n",
      "Train Epoch: 1 [41600/54210 (77%)]\tLoss: 0.966801\n",
      "Train Epoch: 1 [42400/54210 (78%)]\tLoss: 0.774851\n",
      "Train Epoch: 1 [43200/54210 (80%)]\tLoss: 0.791181\n",
      "Train Epoch: 1 [44000/54210 (81%)]\tLoss: 1.046938\n",
      "Train Epoch: 1 [44800/54210 (83%)]\tLoss: 0.704002\n",
      "Train Epoch: 1 [45600/54210 (84%)]\tLoss: 1.511362\n",
      "Train Epoch: 1 [46400/54210 (86%)]\tLoss: 0.428591\n",
      "Train Epoch: 1 [47200/54210 (87%)]\tLoss: 0.681544\n",
      "Train Epoch: 1 [48000/54210 (89%)]\tLoss: 0.874340\n",
      "Train Epoch: 1 [48800/54210 (90%)]\tLoss: 0.215831\n",
      "Train Epoch: 1 [49600/54210 (91%)]\tLoss: 1.376625\n",
      "Train Epoch: 1 [50400/54210 (93%)]\tLoss: 0.697669\n",
      "Train Epoch: 1 [51200/54210 (94%)]\tLoss: 0.565047\n",
      "Train Epoch: 1 [52000/54210 (96%)]\tLoss: 0.601071\n",
      "Train Epoch: 1 [52800/54210 (97%)]\tLoss: 0.904996\n",
      "Train Epoch: 1 [53600/54210 (99%)]\tLoss: 0.718115\n",
      "\n",
      "Test set: Average loss: 0.0432, Accuracy: 8713/9796 (89%)\n",
      "\n",
      "Precision: 0.8980, Recall: 0.8879\n",
      "Confusion Matrix:\n",
      " [[943   0   3  15   1   0   9   1   8   0]\n",
      " [  1 935   5   1   4   1  10   1  44   0]\n",
      " [  5   0 861  69   7  25  29   0   6   0]\n",
      " [  4   0   9 944   0  29   4   7   4   1]\n",
      " [  0   1   3   0 925   3   8   0  13  29]\n",
      " [  3   1  15 225   0 636   2   8   2   0]\n",
      " [ 13   3  23  16   9   9 878   0   7   0]\n",
      " [  2   6  13  17   2  26   0 921  13   2]\n",
      " [  3   0   5  43   0  22  15   1 884   1]\n",
      " [  7   1   2  35  19  19   1  57  75 786]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import resnet50\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "import wandb\n",
    "\n",
    "# 初始化 wandb\n",
    "wandb.init(\n",
    "    project=\"my-ResNet50-project\",\n",
    "    config={\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 8,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 设定设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据转换，适用于彩色图像\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   # 调整图像大小以匹配模型输入\n",
    "    transforms.RandomHorizontalFlip(),  # 随机水平翻转\n",
    "    transforms.RandomRotation(10),  # 随机旋转\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.2197, 0.2214, 0.2221], std=[0.0745, 0.0771, 0.0801]),\n",
    "])\n",
    "\n",
    "\n",
    "# 定义自定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.annotations.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 加载数据集\n",
    "train_csv = '/root/autodl-tmp/xin/GAN/CGAN/data/mixed_images82secondgen.csv'\n",
    "test_csv = '/root/autodl-tmp/xin/datasets/MNIST/updated_test_labels_with_colors.csv'\n",
    "dataset_path = '/root/autodl-tmp/xin/GAN/CGAN/data/mixed_images82secondgen'\n",
    "testdatapath = \"/root/autodl-tmp/xin/datasets/MNIST/colored-test-images\"\n",
    "train_dataset = CustomDataset(csv_file=train_csv, root_dir=dataset_path, transform=transform)\n",
    "test_dataset = CustomDataset(csv_file=test_csv, root_dir=testdatapath, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "# 加载预训练模型\n",
    "model = resnet50(weights='IMAGENET1K_V1').to(device)\n",
    "\n",
    "num_features = model.fc.in_features  # 获取最后一个线性层的输入特征数量\n",
    "model.fc = nn.Linear(num_features, 10).to(device)  # 替换为新的线性层，适配10类\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练函数\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging to wandb\n",
    "        wandb.log({\"Train Loss\": loss.item()})\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "        torch.cuda.empty_cache()  # 尝试在这里清理CUDA缓存\n",
    "\n",
    "# 测试函数\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # 将一批的损失加和\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # 获取概率最高的索引\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    # 计算混淆矩阵及其它统计指标\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds, average='macro')\n",
    "    recall = recall_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    # Logging to wandb\n",
    "    wandb.log({\"Test Loss\": test_loss, \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall})\n",
    "    \n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)\\n')\n",
    "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "    print('Confusion Matrix:\\n', cm)\n",
    "\n",
    "# 保存模型函数\n",
    "def save_model(model, model_name, epoch, path):\n",
    "    torch.save(model.state_dict(), os.path.join(path, f\"{model_name}_epoch{epoch}.pth\"))\n",
    "\n",
    "model_save_path = '/root/autodl-tmp/xin/Classify/modelweight'\n",
    "\n",
    "# 训练和测试模型\n",
    "for epoch in range(1, wandb.config.epochs + 1):\n",
    "    train(epoch)\n",
    "    test()\n",
    "    save_model(model, \"resnet50secondhun10%\", epoch, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All confusion matrices saved.\n",
      "Results saved to /root/autodl-tmp/xin/Classify/results/classification_accuracy_by_color_82resnet50secondhun10%_epoch1.pth.csv\n",
      "Plot saved to /root/autodl-tmp/xin/Classify/results/classification_accuracy_by_color_resnet50secondhun10%_epoch1.pth.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# 定义自定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.annotations.iloc[idx, 1]\n",
    "        color = self.annotations.iloc[idx, 2]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label, color\n",
    "\n",
    "# 数据转换，适用于彩色图像\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.0439, 0.0438, 0.0438], std=[0.0942, 0.0941, 0.0941]),原数据集\n",
    "    # transforms.Normalize(mean=[0.1362, 0.1372, 0.1376], std=[0.0834, 0.0848, 0.0867]),第一次混合后的数据集\n",
    "    transforms.Normalize(mean=[0.2197, 0.2214, 0.2221], std=[0.0745, 0.0771, 0.0801]), \n",
    "    \n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "test_csv = '/root/autodl-tmp/xin/datasets/MNIST/updated_test_labels_with_colors.csv'\n",
    "testdatapath = \"/root/autodl-tmp/xin/datasets/MNIST/colored-test-images\"\n",
    "test_dataset = CustomDataset(csv_file=test_csv, root_dir=testdatapath, transform=transform)\n",
    "\n",
    "# 调整批量大小\n",
    "batch_size = 4  # 调整为更小的批量大小以减少内存使用\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# 加载预训练模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet50(weights='IMAGENET1K_V1').to(device)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 10).to(device)\n",
    "\n",
    "# 加载已经训练好的模型权重\n",
    "# model_weight_name = 'resnet50hun10%_epoch1.pth'\n",
    "model_weight_name = \"resnet50secondhun10%_epoch1.pth\"\n",
    "model_save_path = f'/root/autodl-tmp/xin/Classify/modelweight/{model_weight_name}'\n",
    " \n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "model.eval()\n",
    "\n",
    "# 获取预测结果和实际标签\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "all_colors = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target, color in test_loader:\n",
    "        data, target, color = data.to(device), target.to(device), color.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True).view(-1)\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_targets.extend(target.cpu().numpy())\n",
    "        all_colors.extend(color.cpu().numpy())\n",
    "\n",
    "# 转换为NumPy数组\n",
    "all_preds = np.array(all_preds)\n",
    "all_targets = np.array(all_targets)\n",
    "all_colors = np.array(all_colors)\n",
    "\n",
    "# 为每种颜色生成和保存混淆矩阵\n",
    "color_map = {0: 'Red', 1: 'Green', 2: 'Blue'}\n",
    "result_save_path = '/root/autodl-tmp/xin/Classify/results'\n",
    "os.makedirs(result_save_path, exist_ok=True)\n",
    "\n",
    "for color, color_name in color_map.items():\n",
    "    mask = (all_colors == color)\n",
    "    preds_color = all_preds[mask]\n",
    "    targets_color = all_targets[mask]\n",
    "    cm = confusion_matrix(targets_color, preds_color)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix for {color_name}')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(os.path.join(result_save_path, f'confusion_matrix_{color_name}_{model_weight_name}.png'))\n",
    "    plt.close()\n",
    "\n",
    "print(\"All confusion matrices saved.\")\n",
    "\n",
    "\n",
    "\n",
    "# 计算每个数字在每种颜色下的分类成功率\n",
    "unique_labels = np.unique(all_targets)\n",
    "unique_colors = np.unique(all_colors)\n",
    "\n",
    "# 创建结果保存目录\n",
    "result_save_path = '/root/autodl-tmp/xin/Classify/results'\n",
    "os.makedirs(result_save_path, exist_ok=True)\n",
    "\n",
    "# 存储结果的DataFrame\n",
    "results = []\n",
    "\n",
    "for label in unique_labels:\n",
    "    for color in unique_colors:\n",
    "        # 过滤出当前数字和颜色的样本\n",
    "        mask = (all_targets == label) & (all_colors == color)\n",
    "        if np.sum(mask) == 0:\n",
    "            continue\n",
    "        true_positive = np.sum((all_preds[mask] == label))\n",
    "       \n",
    "        accuracy = true_positive / np.sum(mask)\n",
    "        results.append({\"Digit\": label, \"Color\": color, \"Accuracy\": accuracy})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 保存结果到CSV文件\n",
    "results_csv_path = os.path.join(result_save_path, f'classification_accuracy_by_color_82{model_weight_name}.csv')\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"Results saved to {results_csv_path}\")\n",
    "\n",
    "# 可视化结果并保存图像\n",
    "plt.figure(figsize=(10, 8))\n",
    "for label in unique_labels:\n",
    "    subset = results_df[results_df[\"Digit\"] == label]\n",
    "    plt.plot(subset[\"Color\"], subset[\"Accuracy\"], marker='o', label=f\"Digit {label}\")\n",
    "\n",
    "plt.xlabel(\"Color\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Classification Accuracy for Each Digit by Color\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 保存图像到文件\n",
    "accuracy_plot_path = os.path.join(result_save_path, f'classification_accuracy_by_color_{model_weight_name}.png')\n",
    "plt.savefig(accuracy_plot_path)\n",
    "print(f\"Plot saved to {accuracy_plot_path}\")\n",
    "plt.close()  # 关闭绘图以释放内存\n",
    "\n",
    "# 绘制并保存混淆矩阵\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "def plot_confusion_matrix(cm, labels, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(result_save_path, f'confusion_matrix_{model_weight_name}.png'))\n",
    "    plt.close()\n",
    "\n",
    "labels = [str(i) for i in range(10)]  # 假设标签为0到9\n",
    "plot_confusion_matrix(cm, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
